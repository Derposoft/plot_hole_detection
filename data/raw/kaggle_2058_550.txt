
I'm not the best programmer in the world, but I'm pretty good. See, I can tell what a piece of code is going to cost at a glance. It's surprisingly high. A single line of boilerplate might be a few cents, taking no time at all to write and test. A single line in the middle of a hot loop might cost far north of $1000, or even $10,000, with all the optimization and care that goes into it. It's just something that comes to me. I don't know how I know, but I do. It's actually a pretty effective way to find serious bugs. If there's an otherwise unremarkable line, nestled in a field of $1-$15 lines, that has a future price tag of $40,000, it's a good bet that's a line that needs fixing. It's usually something that would potentially grind production to a halt, or lose massive amounts of user data. It's not always effective, but it's a good first scan for glitches. One time though. One time I saw a line that had a price tag that just shocked me. The number was somewhere in the *trillions* of dollars. Accountants will tell you the value of a year of human life in cold hard dollars is somewhere around $129,000. You don't get a price tag as high as $80.4 *trillion* dollars without people dying. I have no idea why, the line itself was a debug statement: `printf("%dn", x);` as bog standard of a line as you can get. Absolutely chilling. So, I deleted it. Nothing's gone wrong so far! Thank goodness no code is written in stone! *** Edit: This, dear friends, is an excellent example of why literary analysis is complete bunk, and, at the same time, why the death of the author is so important. While I am a programmer, I am not a *C* programmer. I just picked a short looking line of code in a language I knew was dangerous. Frankly, in the domains I work in, relying on stdout for *anything* important, besides dumb logs, seems silly to me. (Though, to be fair, you *should* still be decorating your logs.) Make a dedicated socket (or MPSC, or whatever flavor of dedicated channel you like) if you have to transfer actual critical data. CLIs are different, yes, but that's a very special class of program. Anyways, I didn't mean anything particular by picking a printf, other than that debug statements sometimes are the cause of [heisenbugs](http://www.catb.org/~esr/jargon/html/H/heisenbug.html), and can be difficult to trace. I absolutely am not familiar enough with C printf syntax to intentionally pick a decimal format to imply decimation. Lacking (almost) any form of type safety and having undefined behavior are the banner and seal of C as a language, not just the printf statements. 80 trillion was a number I generated on RANDOM.org, not a particularly deliberate selection, though I did intend it to represent global collapse in a vague way. Finally, Rust is an *amazing* language, but it won't save you from a poorly chosen print! statement. That said, all of your additions, while not my intent, have only served to enrich the story, and make me seem *even smarter than I am*. You're all wrong, and you're all right. Except the hyperinflation guy. He was exactly right. 